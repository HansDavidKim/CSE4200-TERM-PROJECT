# -*- coding: utf-8 -*-
"""wrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P2IcTRM-UKjgqIyO5SXD4zCTtX2CVzJX
"""

# coding=utf-8
# CleanRL Compatible Wrappers for RecSim

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, Any, Tuple, Optional
from gymnasium.spaces import Box
from gymnasium.core import ObservationWrapper

class FlattenObservationWrapper(gym.ObservationWrapper):
    """
    RecSim의 Dict observation을 flat vector로 변환.
    user, doc, response를 모두 포함합니다.
    CleanRL의 PPO/SAC는 Box observation을 기대합니다.
    """

    def __init__(self, env):
        super().__init__(env)

        # 모든 observation keys 포함
        self.obs_keys = ['user', 'doc', 'response']
        self.obs_shapes = {}
        self.obs_dtypes = {}

        # 각 key의 shape과 dtype 저장
        for key in self.obs_keys:
            space = self.env.observation_space[key]
            if isinstance(space, spaces.Box):
                self.obs_shapes[key] = space.shape
                self.obs_dtypes[key] = space.dtype
            elif isinstance(space, spaces.Tuple):
                # Multi-user 또는 multi-response의 경우
                self.obs_shapes[key] = self._get_tuple_shape(space)
                self.obs_dtypes[key] = np.float32
            elif space is None or (hasattr(space, '__class__') and space.__class__.__name__ == 'NoneType'):
                # response가 None일 수 있음 (reset 직후)
                # response space의 크기를 미리 계산
                if key == 'response':
                    self.obs_shapes[key] = self._get_response_shape()
                    self.obs_dtypes[key] = np.float32
            else:
                raise NotImplementedError(f"Unsupported space type for {key}: {type(space)}")

        # 전체 observation dimension 계산
        total_dim = sum(np.prod(shape) for shape in self.obs_shapes.values())

        # Flattened observation space
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(total_dim,),
            dtype=np.float32
        )

        # response가 None일 때 사용할 zero padding 크기
        self.response_dim = np.prod(self.obs_shapes['response'])

    def _get_tuple_shape(self, tuple_space):
        """Tuple space의 총 shape 계산"""
        total_size = 0
        for subspace in tuple_space.spaces:
            if isinstance(subspace, spaces.Box):
                total_size += np.prod(subspace.shape)
            elif isinstance(subspace, spaces.Dict):
                # Dict space의 경우 모든 값의 크기 합산
                for dict_subspace in subspace.spaces.values():
                    if isinstance(dict_subspace, spaces.Box):
                        total_size += np.prod(dict_subspace.shape)
                    else:
                        total_size += 1  # scalar
            elif isinstance(subspace, spaces.Tuple):
                # Nested tuple (response의 경우)
                total_size += self._get_tuple_shape(subspace)[0]
            else:
                total_size += 1  # scalar 값으로 가정
        return (total_size,)

    def _get_response_shape(self):
        """Response space의 shape을 미리 계산"""
        # env의 원본 observation space에서 response space 추출
        response_space = self.env.unwrapped.observation_space['response']

        if response_space is None:
            # Single user의 경우, user_model의 response_space 사용
            from recsim.simulator import environment
            if isinstance(self.env.unwrapped._environment, environment.MultiUserEnvironment):
                base_resp = self.env.unwrapped._environment.user_model[0].response_space()
                num_users = self.env.unwrapped._environment.num_users
                slate_size = self.env.unwrapped._environment.slate_size

                # Multi-user: (num_users, slate_size, response_dim)
                single_resp_dim = self._get_space_dim(base_resp)
                total_dim = num_users * slate_size * single_resp_dim
            else:
                resp_space = self.env.unwrapped._environment.user_model.response_space()
                slate_size = self.env.unwrapped._environment.slate_size

                # Single user: (slate_size, response_dim)
                single_resp_dim = self._get_space_dim(resp_space)
                total_dim = slate_size * single_resp_dim

            return (total_dim,)
        else:
            return self._get_tuple_shape(response_space)

    def _get_space_dim(self, space):
        """단일 space의 dimension 계산"""
        if isinstance(space, spaces.Box):
            return int(np.prod(space.shape))
        elif isinstance(space, spaces.Dict):
            total = 0
            for subspace in space.spaces.values():
                total += self._get_space_dim(subspace)
            return total
        elif isinstance(space, spaces.Discrete):
            return 1
        else:
            return 1

    def _flatten_value(self, value, key):
        """값을 flat array로 변환"""
        if value is None:
            # response가 None인 경우 (reset 직후)
            if key == 'response':
                return np.zeros(self.response_dim, dtype=np.float32)
            else:
                raise ValueError(f"Unexpected None value for key: {key}")

        if isinstance(value, tuple):
            # Multi-user 또는 multi-response의 경우
            flattened = []
            for v in value:
                if isinstance(v, tuple):
                    # Nested tuple (multi-user의 각 user response)
                    for vv in v:
                        flattened.append(self._flatten_single_value(vv))
                else:
                    flattened.append(self._flatten_single_value(v))
            return np.concatenate(flattened)
        else:
            return self._flatten_single_value(value)

    def _flatten_single_value(self, value):
        """단일 값을 flatten"""
        if isinstance(value, np.ndarray):
            return value.flatten()
        elif isinstance(value, dict):
            # Dict의 모든 values를 flatten
            flattened = []
            for v in value.values():
                if isinstance(v, np.ndarray):
                    flattened.append(v.flatten())
                else:
                    flattened.append(np.atleast_1d(v).flatten())
            return np.concatenate(flattened)
        elif isinstance(value, (int, float, np.integer, np.floating)):
            return np.array([value], dtype=np.float32)
        elif hasattr(value, '__dict__'):
            # Object인 경우 __dict__의 값들을 사용
            flattened = []
            for v in value.__dict__.values():
                if isinstance(v, np.ndarray):
                    flattened.append(v.flatten())
                else:
                    flattened.append(np.atleast_1d(v).flatten())
            return np.concatenate(flattened) if flattened else np.array([], dtype=np.float32)
        else:
            return np.atleast_1d(value).flatten()

    def observation(self, obs: dict) -> np.ndarray:
        """
        Dict observation을 flat vector로 변환.
        user, doc, response를 모두 포함합니다.

        Args:
            obs: {'user': ..., 'doc': ..., 'response': ...}

        Returns:
            Flattened numpy array
        """
        flattened_parts = []

        # 순서를 일관되게 유지: user -> doc -> response
        for key in self.obs_keys:
            if key in obs:
                try:
                    flattened = self._flatten_value(obs[key], key)
                    flattened_parts.append(flattened)
                except Exception as e:
                    print(f"Error flattening {key}: {e}")
                    print(f"Value type: {type(obs[key])}")
                    if obs[key] is not None:
                        print(f"Value: {obs[key]}")
                    raise

        # 모든 부분을 연결
        result = np.concatenate(flattened_parts).astype(np.float32)

        # Dimension 체크
        expected_dim = self.observation_space.shape[0]
        if result.shape[0] != expected_dim:
            raise ValueError(
                f"Flattened observation dimension mismatch: "
                f"expected {expected_dim}, got {result.shape[0]}"
            )

        return result

    def reset(self, **kwargs):
        """Reset with proper response handling"""
        obs, info = self.env.reset(**kwargs)

        # response가 None이면 zero padding 추가
        if obs.get('response') is None:
            obs['response'] = None

        return self.observation(obs), info


# 테스트 및 디버깅을 위한 헬퍼 함수
def print_observation_structure(env, obs):
    """Observation 구조를 출력하는 디버깅 함수"""
    print("\n=== Observation Structure ===")
    for key in ['user', 'doc', 'response']:
        if key in obs:
            value = obs[key]
            print(f"\n{key}:")
            print(f"  Type: {type(value)}")

            if value is None:
                print(f"  Value: None")
            elif isinstance(value, np.ndarray):
                print(f"  Shape: {value.shape}")
                print(f"  Dtype: {value.dtype}")
            elif isinstance(value, tuple):
                print(f"  Tuple length: {len(value)}")
                for i, v in enumerate(value):
                    if isinstance(v, tuple):
                        print(f"    [{i}] Nested tuple length: {len(v)}")
                        for j, vv in enumerate(v):
                            _print_value_info(vv, f"      [{i}][{j}]")
                    else:
                        _print_value_info(v, f"    [{i}]")
            elif isinstance(value, dict):
                print(f"  Dict keys: {value.keys()}")
                for k, v in value.items():
                    _print_value_info(v, f"    {k}")


def _print_value_info(value, prefix=""):
    """값의 정보를 출력"""
    if isinstance(value, np.ndarray):
        print(f"{prefix} ndarray: shape={value.shape}, dtype={value.dtype}")
    elif isinstance(value, dict):
        print(f"{prefix} dict with keys: {value.keys()}")
    elif isinstance(value, (int, float)):
        print(f"{prefix} scalar: {value}")
    else:
        print(f"{prefix} type: {type(value)}")

# 사용할 CleanRL 알고리즘이 MultiDiscrete 액션을 직접 지원한다면 이 래퍼는 불필요
class DiscreteToMultiBinaryWrapper(gym.ActionWrapper):
    """
    MultiDiscrete action을 MultiBinary로 변환.
    일부 알고리즘에서 유용할 수 있습니다.
    """

    def __init__(self, env):
        super().__init__(env)

        if isinstance(env.action_space, spaces.MultiDiscrete):
            self.nvec = env.action_space.nvec
            total_bits = int(np.sum(np.ceil(np.log2(self.nvec))))
            self.action_space = spaces.MultiBinary(total_bits)
        else:
            raise ValueError("This wrapper only supports MultiDiscrete action spaces")

    def action(self, action):
        """MultiBinary action을 MultiDiscrete로 변환"""
        discrete_action = []
        bit_idx = 0

        for n in self.nvec:
            n_bits = int(np.ceil(np.log2(n)))
            bits = action[bit_idx:bit_idx + n_bits]
            value = sum(b * (2 ** i) for i, b in enumerate(bits))
            discrete_action.append(min(value, n - 1))
            bit_idx += n_bits

        return np.array(discrete_action)

class NormalizeRewardWrapper(gym.RewardWrapper):
    """
    Reward를 정규화하여 학습 안정성 향상.
    Running mean/std를 사용합니다.
    누적 할인 보상(Discounted Return)의 실행 평균/분산(Running Mean/Var)을 기반으로 보상을 정규화.
    """

    def __init__(self, env, gamma=0.99, epsilon=1e-8):
        super().__init__(env)
        self.gamma = gamma
        self.epsilon = epsilon
        self.return_val = 0.0
        self.return_mean = 0.0
        self.return_var = 1.0
        self.count = 0

    def reward(self, reward):
        """Reward 정규화"""
        self.return_val = reward + self.gamma * self.return_val
        self.count += 1

        # Welford's online algorithm for running mean/var
        delta = self.return_val - self.return_mean
        self.return_mean += delta / self.count
        delta2 = self.return_val - self.return_mean
        self.return_var += delta * delta2

        std = np.sqrt(self.return_var / self.count + self.epsilon)
        normalized_reward = reward / std

        return normalized_reward

    def reset(self, **kwargs):
        self.return_val = 0.0
        return super().reset(**kwargs)

class RecordEpisodeStatisticsWrapper(gym.Wrapper):
    """
    에피소드 통계를 기록하는 래퍼.
    CleanRL의 로깅과 호환됩니다.
    에피소드가 종료될 때 총 보상(r)과 길이(l)를 딕셔너리에 기록합니다.
    """

    def __init__(self, env):
        super().__init__(env)
        self.episode_returns = []
        self.episode_lengths = []
        self.episode_return = 0.0
        self.episode_length = 0

    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)

        self.episode_return += reward
        self.episode_length += 1

        if terminated or truncated:
            info['episode'] = {
                'r': self.episode_return,
                'l': self.episode_length,
            }
            self.episode_returns.append(self.episode_return)
            self.episode_lengths.append(self.episode_length)

        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        self.episode_return = 0.0
        self.episode_length = 0
        return super().reset(**kwargs)

class CleanRLRecSimWrapper(gym.Wrapper):
    """
    CleanRL을 위한 올인원 래퍼.
    필요한 모든 변환을 한 번에 적용합니다.
    """

    def __init__(
        self,
        env,
        flatten_obs: bool = True,
        normalize_reward: bool = False,
        record_stats: bool = True,
        gamma: float = 0.99,
    ):
        """
        Args:
            env: RecSimGymEnv 인스턴스
            flatten_obs: observation을 flatten할지 여부
            normalize_reward: reward를 정규화할지 여부
            record_stats: 에피소드 통계를 기록할지 여부
            gamma: reward 정규화에 사용할 discount factor
        """
        super().__init__(env)

        # 래퍼 체인 구성
        current_env = env

        if flatten_obs:
            current_env = FlattenObservationWrapper(current_env)

        if normalize_reward:
            current_env = NormalizeRewardWrapper(current_env, gamma=gamma)

        if record_stats:
            current_env = RecordEpisodeStatisticsWrapper(current_env)

        self.env = current_env
        self.observation_space = current_env.observation_space
        self.action_space = current_env.action_space

    def step(self, action):
        return self.env.step(action)

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

# Vectorized Environment를 위한 헬퍼 : 여러 개의 환경 인스턴스를 동시에 실행 가능
def make_recsim_env(
    raw_environment,
    reward_aggregator,
    flatten_obs: bool = True,
    normalize_reward: bool = False,
    record_stats: bool = True,
    gamma: float = 0.99,
):
    """
    CleanRL 호환 RecSim 환경을 생성하는 팩토리 함수.

    Example:
        >>> from gymnasium.vector import SyncVectorEnv
        >>>
        >>> def make_env():
        >>>     env = RecSimGymEnv(raw_env, reward_agg)
        >>>     env = CleanRLRecSimWrapper(env)
        >>>     return env
        >>>
        >>> envs = SyncVectorEnv([make_env for _ in range(4)])
    """
    from recsim_gym import RecSimGymEnv

    def _init():
        env = RecSimGymEnv(raw_environment, reward_aggregator)
        env = CleanRLRecSimWrapper(
            env,
            flatten_obs=flatten_obs,
            normalize_reward=normalize_reward,
            record_stats=record_stats,
            gamma=gamma,
        )
        return env

    return _init

# CleanRL PPO 사용 예시
def create_ppo_compatible_env(raw_environment, reward_aggregator, num_envs=4):
    """
    CleanRL PPO를 위한 vectorized environment 생성.

    Args:
        raw_environment: RecSim raw environment
        reward_aggregator: reward aggregation function
        num_envs: 병렬 환경 개수

    Returns:
        SyncVectorEnv 인스턴스
    """
    from gymnasium.vector import SyncVectorEnv
    from recsim_gym import RecSimGymEnv

    def make_env(seed):
        def _init():
            env = RecSimGymEnv(raw_environment, reward_aggregator)
            env = CleanRLRecSimWrapper(
                env,
                flatten_obs=True,
                normalize_reward=True,
                record_stats=True,
            )
            env.reset(seed=seed)
            return env
        return _init

    envs = SyncVectorEnv([make_env(i) for i in range(num_envs)])
    return envs


# CleanRL SAC 사용 예시 (continuous action이 필요한 경우)
# MultiDiscrete (Categorical) 액션을 직접 지원하는 SAC 구현체를 사용한다면 이 래퍼는 불필요
class ContinuousActionWrapper(gym.ActionWrapper):
    """
    Discrete action을 continuous로 변환 (SAC용).
    Gumbel-Softmax 등을 사용할 수 있습니다.
    """

    def __init__(self, env, temperature=1.0):
        super().__init__(env)

        if isinstance(env.action_space, spaces.MultiDiscrete):
            self.nvec = env.action_space.nvec
            total_dim = int(np.sum(self.nvec))
            self.action_space = spaces.Box(
                low=-1.0,
                high=1.0,
                shape=(total_dim,),
                dtype=np.float32
            )
            self.temperature = temperature
        else:
            raise ValueError("Only supports MultiDiscrete action spaces")

    def action(self, action):
        """Continuous action을 discrete로 변환 (Gumbel-Softmax)"""
        discrete_actions = []
        idx = 0

        for n in self.nvec:
            logits = action[idx:idx + n]
            # Softmax with temperature
            probs = np.exp(logits / self.temperature)
            probs = probs / np.sum(probs)
            # Sample
            discrete_action = np.random.choice(n, p=probs)
            discrete_actions.append(discrete_action)
            idx += n

        return np.array(discrete_actions)
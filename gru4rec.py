# -*- coding: utf-8 -*-
"""GRU4REC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VFXBR3xawq9WCB6C9Cnzmz8J84lyzUpr
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Tuple
import math
import time

# Session-Parallel 학습을 위한 iterator
class SessionIterator:
    def __init__(self, sessions: List[List[int]], batch_size: int, max_len: int = 20):
        self.batch_size = batch_size
        self.max_len = max_len

        # 세션을 길이 순으로 정렬 -> 비슷한 길이의 세션을 한 배치에 모으기(패딩 최소화)
        # self.sessions = sorted(sessions, key=len, reverse=True)
        # 각 세션에 고유 인덱스 부여
        indexed_sessions = [(idx, session) for idx, session in enumerate(sessions)]
        self.sessions_with_id = sorted(indexed_sessions, key=lambda x: len(x[1]), reverse=True)

        # Session-Parallel을 위해 세션을 일련의 (Input, Target) 시퀀스로 변환
        # self.session_map = self._create_session_map(self.sessions)
        self.session_map = self._create_session_map(self.sessions_with_id) # (ID, Session) 쌍 리스트 전달
        self.batches = self._create_batches(self.session_map, batch_size)
        self.num_batches = len(self.batches)
        self.batch_ptr = 0

    # def _create_session_map(self, sessions):
    def _create_session_map(self, sessions_with_id):
        # (Session Index) -> (Input Sequence List, Target Sequence List, Length)
        session_map = {}
        # for idx, session in enumerate(sessions): # 각 session 순회
        # 원본 데이터에서의 고유 인덱스를 키로 사용
        for session_id, session in sessions_with_id: # (고유 ID, 세션 데이터) 튜플을 바로 언패킹
            if len(session) < 2: continue

            # Input: i_1, i_2, ..., i_{T-1}
            input_seq = session[:-1]
            # Target: i_2, i_3, ..., i_T
            target_seq = session[1:]

            # Max_len 처리 (max_len 초과 시 가장 최근 항목들로 truncate)
            if len(input_seq) > self.max_len:
                input_seq = input_seq[-self.max_len:]
                target_seq = target_seq[-self.max_len:]

            # 맵의 키(Key)로 정렬 후 인덱스(idx) 대신 원본 고유 ID(session_id) 사용
            session_map[session_id] = {
                'input': input_seq,
                'target': target_seq,
                'length': len(input_seq),
                # 이 필드에 고유 ID를 다시 저장
                'session_id': session_id
            }
        return session_map

    def _create_batches(self, session_map, batch_size):
        # 모든 세션 인덱스를 포함하는 리스트를 생성
        session_indices = list(session_map.keys())

        # 세션 인덱스를 batch_size 단위로 묶기
        batches = []
        for i in range(0, len(session_indices), batch_size):
            batches.append(session_indices[i:i + batch_size])

        return batches

    def __iter__(self):
        self.batch_ptr = 0
        return self

    def __next__(self): # 다음 배치 데이터 가져오기
        if self.batch_ptr >= self.num_batches:
            raise StopIteration

        # 현재 배치를 구성할 세션 인덱스 가져오기
        current_session_indices = self.batches[self.batch_ptr]
        self.batch_ptr += 1

        # 해당 세션 데이터 로드
        batch_data = [self.session_map[idx] for idx in current_session_indices]

        # 배치 내에서 가장 긴 시퀀스 길이 찾기
        max_batch_len = max(item['length'] for item in batch_data)

        # 데이터 패딩
        batch_input = []
        batch_target = []
        batch_length = []

        for item in batch_data:
            input_seq = item['input']
            target_seq = item['target']

            # 현재 배치에서 가장 긴 길이에 맞춰 패딩
            pad_len = max_batch_len - len(input_seq)

            padded_input = input_seq + [0] * pad_len
            padded_target = target_seq + [0] * pad_len

            batch_input.append(padded_input)
            batch_target.append(padded_target)
            batch_length.append(item['length'])

        return {
            'input': torch.LongTensor(batch_input),    # (B, Max_Batch_L)
            'target': torch.LongTensor(batch_target),  # (B, Max_Batch_L)
            'length': torch.LongTensor(batch_length)   # (B)
        }

    def __len__(self):
        return self.num_batches

class SessionDataLoader:
    # SessionIterator를 반환하는 DataLoader 클래스
    def __init__(self, sessions: List[List[int]], batch_size: int, max_len: int = 20):
        self.sessions = sessions
        self.batch_size = batch_size
        self.max_len = max_len

    def __iter__(self):
        return SessionIterator(self.sessions, self.batch_size, self.max_len)

    def __len__(self):
        # 총 세션 수 / 배치 크기
        num_valid_sessions = sum(1 for session in self.sessions if len(session) >= 2)
        return math.ceil(num_valid_sessions / self.batch_size)

def top1_loss(pos_scores: torch.Tensor, neg_scores: torch.Tensor) -> torch.Tensor:
    """
    Top1-Loss: sum_j sigma(r_j - r_i) + sigma(r_j^2)
    긍정 항목의 점수(r_i)가 모든 부정 항목의 점수(r_j)보다 높도록 모델을 훈련.
    torch.nn.functional.logsigmoid를 사용하여 수치 안정성을 개선.
    Args:
        pos_scores: 긍정 아이템(실제 다음 아이템)의 로짓 (B,)
        neg_scores: 부정 아이템의 로짓 (B, N) # N: num_neg_samples
    Returns:
        Top1-Loss
    """
    # pos_scores: (B, 1), neg_scores: (B, N)
    pos_scores = pos_scores.unsqueeze(-1)

    # 시그모이드 항: log(sigma(r_j - r_i))
    # 긍정 항목 i보다 부정 항목 j의 점수가 더 높을 가능성(r_j - r_i > 0일 때 sigma 값 증가)을 최소화
    # 즉, r_i > r_j가 되도록 강제.
    # log(sigma(x)) = logsigmoid(x)
    # r_j - r_i : neg_scores - pos_scores (B, N)
    rank_diff = neg_scores - pos_scores # 부정 항목 점수와 긍정 항목 점수의 차이
    sig_term = torch.sum(F.logsigmoid(rank_diff), dim=-1) # 마지막 차원(N개 부정 샘플 차원)에 대해 합 -> (B,)

    # 제곱 항: log(sigma(r_j^2))
    # 정규화(regularization) 역할 -> 부정 항목의 점수 r_j가 0에 가까워지도록 유도하여 점수의 스케일을 안정화.
    # log(sigma(r_j^2))
    square_term = torch.sum(F.logsigmoid(neg_scores.pow(2)), dim=-1) # 마지막 차원(N개 부정 샘플 차원)에 대해 합 -> (B,)

    # Top1 Loss = - (sum_j sig_term + sum_j square_term) / B (배치 평균)
    loss = -torch.mean(sig_term + square_term)
    return loss

# GRU 기반 순차적 추천 모델
class GRU4REC(nn.Module):
    def __init__(self, num_items: int, embedding_dim: int=128, hidden_dim: int=256, num_layers: int=1, dropout: float=0.2):
        """
        Args:
            num_items: 전체 아이템 개수
            embedding_dim: 아이템 임베딩 차원
            hidden_dim: GRU hidden state 차원
            num_layers: GRU 레이어 수
            dropout: 드롭아웃 비율
        """
        super(GRU4REC, self).__init__()

        self.num_items = num_items
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # 아이템 임베딩 (0은 패딩용)
        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)

        # GRU 레이어
        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)

        # 출력 레이어
        self.dropout = nn.Dropout(dropout)
        self.output_layer = nn.Linear(hidden_dim, num_items + 1)

        # 가중치 초기화
        self._init_weights()

    def _init_weights(self):
        # 가중치 초기화
        nn.init.xavier_uniform_(self.item_embedding.weight)
        nn.init.xavier_uniform_(self.output_layer.weight)
        nn.init.zeros_(self.output_layer.bias)

    def forward(self, input_seq: torch.Tensor,  hidden: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            input_seq: (batch_size, seq_len) 아이템 시퀀스
            hidden: (num_layers, batch_size, hidden_dim) 초기 hidden state
        Returns:
            logits: (batch_size, seq_len, num_items+1) 각 아이템에 대한 점수
            hidden: (num_layers, batch_size, hidden_dim) 최종 hidden state
        """
        # 임베딩
        embedded = self.item_embedding(input_seq)  # (B, L, E)

        # GRU
        gru_out, hidden = self.gru(embedded, hidden)  # (B, L, H), (num_layers, B, H)

        # 드롭아웃 및 출력
        gru_out = self.dropout(gru_out)
        logits = self.output_layer(gru_out)  # (B, L, num_items+1)

        return logits, hidden

class GRU4RECTrainer:
    # GRU4REC 모델 학습 및 평가
    def __init__(self, model: GRU4REC, learning_rate: float = 1e-3, num_neg_samples: int = 50,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.num_neg_samples = num_neg_samples # Top1-Loss에 사용할 부정 샘플 개수
        self.num_items = model.num_items # 전체 아이템 수

    def train_epoch(self, dataloader: SessionDataLoader) -> float:
        # 한 epoch 학습 (Hidden State를 재사용)
        self.model.train()
        total_loss = 0
        num_batches = 0

        # B = batch_size, L = max_batch_len, H = hidden_dim, N = num_neg_samples

        for batch in dataloader:
            input_seq = batch['input'].to(self.device)   # (B, L)
            target_seq = batch['target'].to(self.device) # (B, L)
            lengths = batch['length']                    # (B)
            batch_size = input_seq.size(0)

            # Forward pass
            self.optimizer.zero_grad()
            logits, _ = self.model(input_seq) # (B, L, num_items+1)

            # Top1 Loss 계산

            # 유효한 예측 (길이 L 내)에 대해서만 Loss 계산을 적용하기 위해 Mask 생성
            max_batch_len = input_seq.size(1)

            # [0, 1, 2, ..., L-1]
            time_steps = torch.arange(max_batch_len, device=self.device).unsqueeze(0) # (1, L)
            # [L1, L2, L3, ...]
            lengths_expanded = lengths.unsqueeze(1) # (B, 1)

            # Mask: time_step < length
            mask = time_steps < lengths_expanded # (B, L)

            # 긍정 아이템(정답)의 로짓 추출: target_seq를 인덱스로 사용
            # last_logits[i, t, target_seq[i, t]]를 계산

            # 1. (B*L, num_items+1)로 펼침
            logits_flat = logits.view(-1, self.num_items + 1)
            target_flat = target_seq.view(-1)

            # 2. 긍정 아이템의 로짓만 추출 (B*L)
            pos_scores_flat = logits_flat[torch.arange(target_flat.size(0), device=self.device), target_flat]

            # 3. 유효한(패딩이 아닌) 예측만 필터링
            valid_mask = target_flat != 0 # 타겟이 패딩(0)이 아닌 경우만 유효
            pos_scores = pos_scores_flat[valid_mask] # (Valid_Count)

            if pos_scores.size(0) == 0:
                continue

            valid_count = pos_scores.size(0)

            # 4. 부정 아이템 샘플링 (Valid_Count, N)
            # 아이템 1부터 num_items까지
            neg_items = torch.randint(1, self.num_items + 1, (valid_count, self.num_neg_samples), device=self.device)

            # 5. 유효한 시점의 로짓만 추출 (Valid_Count, num_items+1)
            valid_logits = logits_flat[valid_mask.nonzero(as_tuple=True)[0]]

            # 6. 부정 아이템의 로짓 추출 (Valid_Count, N)
            neg_scores = torch.gather(valid_logits, 1, neg_items)

            # Top1 Loss 계산
            loss = top1_loss(pos_scores, neg_scores)

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        return total_loss / num_batches

    def evaluate(self, dataloader: SessionDataLoader, k_list: List[int] = [5, 10, 20]) -> dict:
        # 모델 평가 (Recall@K, MRR@K)
        self.model.eval()

        recalls = {k: [] for k in k_list}
        mrrs = {k: [] for k in k_list}

        with torch.no_grad():
            for batch in dataloader:
                input_seq = batch['input'].to(self.device)
                target_seq = batch['target'].to(self.device)
                lengths = batch['length']

                batch_size = input_seq.size(0)

                for i in range(batch_size):
                    seq_len = lengths[i].item()

                    if seq_len < 1:  # 유효한 쌍이 없음
                        continue

                    # 평가에 사용할 유효한 Input Sequence (i_1, ..., i_{T-1})
                    # seq_len은 (Input, Target) 쌍의 길이
                    # Input Sequence의 실제 길이는 seq_len

                    # curr_input: (1, seq_len)
                    # Input Sequence는 i_1, ..., i_{T-1} (길이 L-1)이 아니라,
                    # i_1, ..., i_L 이고, GRU의 출력은 i_2, ..., i_{L+1}에 대한 예측.
                    # dataloader의 input은 (i_1, ..., i_{T-1}) 길이 L, target은 (i_2, ..., i_T) 길이 L.
                    # 우리는 i_1, ..., i_{T-1} 을 넣어 i_T를 예측해야 함.

                    # input_seq[i:i+1, :seq_len]은 (i_1, ..., i_{T-1}). (길이: seq_len)
                    input_to_predict = input_seq[i:i+1, :seq_len] # (1, L)

                    # target_seq[i, seq_len-1]가 최종 정답 i_T.
                    # Target Item은 시퀀스의 마지막 유효한 항목.
                    true_next = target_seq[i, seq_len-1].item() # Target Item (마지막 예측 시점의 정답)

                    if input_to_predict.size(1) == 0 or true_next == 0:
                        continue

                    # curr_input 전체를 forward에 넣어 마지막 타임스텝의 예측을 사용하도록 함
                    # forward()의 출력 logits: (1, L, num_items+1)
                    logits, _ = self.model.forward(input_to_predict)

                    # 마지막 타임스텝의 예측만 사용
                    last_logits = logits[:, -1, :]  # (1, num_items+1)

                    # 패딩 인덱스(0) 제외 (predict_next 내부 로직을 가져옴)
                    last_logits[:, 0] = -float('inf')

                    for k in k_list:
                        # Top-K 선택
                        top_k_scores, top_k_items = torch.topk(last_logits, k, dim=1)
                        top_k = top_k_items[0].cpu().numpy()

                        # Recall@K
                        if true_next in top_k:
                            recalls[k].append(1.0)
                            # MRR@K
                            rank = np.where(top_k == true_next)[0][0] + 1
                            mrrs[k].append(1.0 / rank)
                        else:
                            recalls[k].append(0.0)
                            mrrs[k].append(0.0)

        # 평균 계산
        results = {}
        for k in k_list:
            results[f'Recall@{k}'] = np.mean(recalls[k]) if recalls[k] else 0.0
            results[f'MRR@{k}'] = np.mean(mrrs[k]) if mrrs[k] else 0.0

        return results


    def save_model(self, path: str):
        # 모델 저장
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load_model(self, path: str):
        # 모델 로드
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# RecSim의 에피소드 데이터 : Python List 또는 DataFrame 가정
# SessionDataLoader가 요구하는 세션 리스트 (List[List[int]]) 형태로 변환하고, 아이템 ID 매핑을 처리
class RecSimDataProcessor:
    def __init__(self, episode_data: List[Dict], min_session_len: int = 2):
        """
        Args:
            episode_data: RecSim 시뮬레이션에서 얻은 raw 에피소드 데이터.
                          각 요소는 하나의 타임스텝 Dict:
                          {'doc_id': int, 'user_id': int, ...} 형태를 가정.
            min_session_len: GRU4REC 학습에 사용할 최소 세션 길이.
        """
        self.episode_data = episode_data
        self.min_session_len = min_session_len
        self.item_to_idx = {'<PAD>': 0} # 0은 패딩으로 예약
        self.idx_to_item = {0: '<PAD>'}
        self.next_idx = 1
        self.sessions = []
        self.num_items = 0

    def _map_item_id(self, item_id: int) -> int:
        # 아이템 ID를 정수 인덱스로 매핑
        if item_id not in self.item_to_idx:
            self.item_to_idx[item_id] = self.next_idx
            self.idx_to_item[self.next_idx] = item_id
            self.next_idx += 1
        return self.item_to_idx[item_id]

    def process(self) -> Tuple[List[List[int]], int]:
        """
        RecSim 데이터를 GRU4REC 학습용 세션 리스트로 변환하고 아이템 인덱싱을 수행.
        Returns:
            Tuple[List[List[int]], int]: (세션 리스트, 전체 아이템 개수)
        """
        print("--- RecSim Data Processing Start ---")

        # 로그가 정렬되어있지 않을 경우 정렬
        # 정렬 기준: 1순위 user_id, 2순위 time
        # sorted_data = sorted(
        #     self.episode_data,
        #     key=lambda step: (step.get('user_id'), step.get('time'))
        # )

        # 1. User/Session별로 상호작용 그룹화
        # 'user_id'를 기준으로 그룹화
        # 각 사용자의 모든 상호작용을 하나의 긴 세션으로 간주
        # 'episode_id'가 있다면 이를 활용하여 세션 나누기

        # 가정 1 : episode_data는 Time-Step 순서대로 정렬되어있음
        # 가정 2 : 각 상호작용은 'user_id'와 'doc_id'(아이템 ID)를 포함
        # 가정 3 : 'doc_id'가 클릭된(consumed) 아이템(아닐 경우 아래의 주석처리된 process함수 사용)

        grouped_interactions: Dict[int, List[int]] = {}

        for step in self.episode_data:
            user_id = step.get('user_id')
            doc_id = step.get('doc_id') # 클릭/선택된 아이템 ID

            if user_id is None or doc_id is None:
                continue

            # 아이템 ID 매핑
            mapped_doc_id = self._map_item_id(doc_id)

            if user_id not in grouped_interactions:
                grouped_interactions[user_id] = []

            grouped_interactions[user_id].append(mapped_doc_id)

        # 2. 세션 리스트 생성 및 필터링
        raw_sessions = list(grouped_interactions.values())
        self.sessions = [] # 기존 세션 초기화

        for session in raw_sessions:
            if len(session) >= self.min_session_len:
                self.sessions.append(session)

        self.num_items = self.next_idx - 1 # 0(PAD)을 제외한 실제 아이템 개수

        print(f"Total Raw Interactions Grouped: {len(raw_sessions)}")
        print(f"Valid Sessions (Len >= {self.min_session_len}): {len(self.sessions)}")
        print(f"Total Unique Items (num_items): {self.num_items}")
        print("--- RecSim Data Processing End ---")

        return self.sessions, self.num_items

    def get_item_map(self):
        return self.item_to_idx, self.idx_to_item